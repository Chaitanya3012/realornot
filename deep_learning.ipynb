{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import spacy\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Flatten,Dropout,Input\n",
    "from keras.layers import Embedding,LSTM,GlobalMaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_df = twit.drop(columns = ['id','keyword','location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  target\n",
       "0  Our Deeds are the Reason of this #earthquake M...       1\n",
       "1             Forest fire near La Ronge Sask. Canada       1\n",
       "2  All residents asked to 'shelter in place' are ...       1\n",
       "3  13,000 people receive #wildfires evacuation or...       1\n",
       "4  Just got sent this photo from Ruby #Alaska as ...       1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(twit_df['target'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(len(stopwords.words('english')))\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "326\n"
     ]
    }
   ],
   "source": [
    "print(len(nlp.Defaults.stop_words))\n",
    "stopwords = (nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S\\s+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text) # no emoji\n",
    "\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "    \n",
    "def stop_words(text):\n",
    "    text = \" \".join(word for word in text.split() if word not in stopwords)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_df['text'] = twit_df['text'].apply(remove_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_df['text'] = twit_df['text'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_df['text'] = twit_df['text'].map(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_df['text'] = twit_df['text'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "twit_df['text'] = twit_df['text'].apply(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                deeds reason earthquake allah forgive\n",
       "1                forest fire near la ronge sask canada\n",
       "2    residents asked shelter place notified officer...\n",
       "3    people receive wildfires evacuation orders cal...\n",
       "4    got sent photo ruby alaska smoke wildfires pou...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twit_df['text'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = twit_df['text']\n",
    "y = twit_df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size = 0.3,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1186    ashes australiaûªs collapse trent bridge wors...\n",
      "4071    great michigan technique camp bg thanks bmurph...\n",
      "5461    cnn tennessee movie theater shooting suspect k...\n",
      "5787                      rioting couple hours left class\n",
      "7445    crack path wiped morning beach run surface wou...\n",
      "Name: text, dtype: object\n",
      "(5329,)\n",
      "\n",
      "\n",
      "\n",
      "1186    0\n",
      "4071    1\n",
      "5461    1\n",
      "5787    1\n",
      "7445    0\n",
      "Name: target, dtype: int64\n",
      "\n",
      "\n",
      "\n",
      "2644            new weapon cause unimaginable destruction\n",
      "2227    famping things gishwhes got soaked deluge goin...\n",
      "5448    dt georgegalloway rt gallowaymayor ûïthe col ...\n",
      "132     aftershock school kick great want thank making...\n",
      "6845    response trauma children addicts develop defen...\n",
      "Name: text, dtype: object\n",
      "(2284,)\n",
      "\n",
      "\n",
      "\n",
      "2644    1\n",
      "2227    0\n",
      "5448    1\n",
      "132     0\n",
      "6845    0\n",
      "Name: target, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(X_train.head())\n",
    "print(X_train.shape)\n",
    "print('\\n\\n')\n",
    "\n",
    "print(y_train.head())\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "print(X_val.head())\n",
    "print(X_val.shape)\n",
    "\n",
    "print('\\n\\n')\n",
    "\n",
    "print(y_val.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[760, 3169, 68, 978, 367, 646, 429, 1217, 3170, 699]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13508"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "\n",
    "print(X_train_seq[0])\n",
    "v= len(word2idx)\n",
    "v\n",
    "#print(X_val_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#setting the hyperparameters\n",
    "\n",
    "output_dir = 'model_output/dense'\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "#vector spacing embedding\n",
    "n_dim = 64\n",
    "n_unique_words = 5000\n",
    "n_words_to_skip = 50\n",
    "pad_type = trunc_type = 'pre'\n",
    "\n",
    "#Neural network architecture:\n",
    "\n",
    "n_dense = 64\n",
    "dropout = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize input length by padding and trucating\n",
    "\n",
    "X_train_data = pad_sequences(X_train_seq)\n",
    "\n",
    "T = X_train_data.shape[1]\n",
    "\n",
    "X_val_data= pad_sequences(X_val_seq,maxlen=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2284, 21)\n"
     ]
    }
   ],
   "source": [
    "print(X_val_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "D = 20\n",
    "M=15\n",
    "\n",
    "\n",
    "i = Input(shape=(T,))\n",
    "x = Embedding(v+1,D)(i)\n",
    "x = LSTM(M,return_sequences=True)(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(1,activation = 'sigmoid')(x)\n",
    "\n",
    "model = Model(i,x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 21)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 21, 20)            270180    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 21, 15)            2160      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 16        \n",
      "=================================================================\n",
      "Total params: 272,356\n",
      "Trainable params: 272,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'model_output/dense'\n",
    "\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "5329/5329 [==============================] - ETA: 4s - loss: 0.0332 - accuracy: 1.00 - ETA: 5s - loss: 0.0269 - accuracy: 0.98 - ETA: 5s - loss: 0.0301 - accuracy: 0.98 - ETA: 5s - loss: 0.0287 - accuracy: 0.99 - ETA: 5s - loss: 0.0351 - accuracy: 0.98 - ETA: 5s - loss: 0.0312 - accuracy: 0.99 - ETA: 5s - loss: 0.0280 - accuracy: 0.99 - ETA: 5s - loss: 0.0259 - accuracy: 0.99 - ETA: 5s - loss: 0.0269 - accuracy: 0.99 - ETA: 5s - loss: 0.0287 - accuracy: 0.99 - ETA: 5s - loss: 0.0267 - accuracy: 0.99 - ETA: 5s - loss: 0.0271 - accuracy: 0.99 - ETA: 4s - loss: 0.0290 - accuracy: 0.99 - ETA: 4s - loss: 0.0315 - accuracy: 0.98 - ETA: 4s - loss: 0.0299 - accuracy: 0.99 - ETA: 4s - loss: 0.0300 - accuracy: 0.98 - ETA: 4s - loss: 0.0290 - accuracy: 0.99 - ETA: 4s - loss: 0.0287 - accuracy: 0.99 - ETA: 4s - loss: 0.0300 - accuracy: 0.98 - ETA: 4s - loss: 0.0291 - accuracy: 0.99 - ETA: 4s - loss: 0.0293 - accuracy: 0.99 - ETA: 4s - loss: 0.0287 - accuracy: 0.99 - ETA: 4s - loss: 0.0282 - accuracy: 0.99 - ETA: 4s - loss: 0.0279 - accuracy: 0.99 - ETA: 4s - loss: 0.0274 - accuracy: 0.99 - ETA: 4s - loss: 0.0274 - accuracy: 0.99 - ETA: 3s - loss: 0.0279 - accuracy: 0.99 - ETA: 3s - loss: 0.0276 - accuracy: 0.99 - ETA: 3s - loss: 0.0282 - accuracy: 0.99 - ETA: 3s - loss: 0.0287 - accuracy: 0.99 - ETA: 3s - loss: 0.0285 - accuracy: 0.99 - ETA: 3s - loss: 0.0285 - accuracy: 0.99 - ETA: 3s - loss: 0.0312 - accuracy: 0.98 - ETA: 3s - loss: 0.0314 - accuracy: 0.98 - ETA: 3s - loss: 0.0306 - accuracy: 0.98 - ETA: 3s - loss: 0.0318 - accuracy: 0.98 - ETA: 3s - loss: 0.0320 - accuracy: 0.98 - ETA: 3s - loss: 0.0317 - accuracy: 0.98 - ETA: 3s - loss: 0.0319 - accuracy: 0.98 - ETA: 3s - loss: 0.0317 - accuracy: 0.98 - ETA: 3s - loss: 0.0313 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.98 - ETA: 2s - loss: 0.0303 - accuracy: 0.98 - ETA: 2s - loss: 0.0302 - accuracy: 0.98 - ETA: 2s - loss: 0.0299 - accuracy: 0.98 - ETA: 2s - loss: 0.0297 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.98 - ETA: 2s - loss: 0.0317 - accuracy: 0.98 - ETA: 2s - loss: 0.0327 - accuracy: 0.98 - ETA: 2s - loss: 0.0326 - accuracy: 0.98 - ETA: 2s - loss: 0.0324 - accuracy: 0.98 - ETA: 2s - loss: 0.0321 - accuracy: 0.98 - ETA: 2s - loss: 0.0319 - accuracy: 0.98 - ETA: 2s - loss: 0.0327 - accuracy: 0.98 - ETA: 2s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0330 - accuracy: 0.98 - ETA: 1s - loss: 0.0328 - accuracy: 0.98 - ETA: 1s - loss: 0.0324 - accuracy: 0.98 - ETA: 1s - loss: 0.0319 - accuracy: 0.98 - ETA: 1s - loss: 0.0315 - accuracy: 0.98 - ETA: 1s - loss: 0.0320 - accuracy: 0.98 - ETA: 1s - loss: 0.0316 - accuracy: 0.98 - ETA: 1s - loss: 0.0317 - accuracy: 0.98 - ETA: 1s - loss: 0.0315 - accuracy: 0.98 - ETA: 1s - loss: 0.0316 - accuracy: 0.98 - ETA: 1s - loss: 0.0318 - accuracy: 0.98 - ETA: 1s - loss: 0.0323 - accuracy: 0.98 - ETA: 1s - loss: 0.0326 - accuracy: 0.98 - ETA: 1s - loss: 0.0322 - accuracy: 0.98 - ETA: 0s - loss: 0.0320 - accuracy: 0.98 - ETA: 0s - loss: 0.0321 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0321 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0316 - accuracy: 0.98 - ETA: 0s - loss: 0.0317 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.98 - ETA: 0s - loss: 0.0318 - accuracy: 0.98 - ETA: 0s - loss: 0.0319 - accuracy: 0.98 - ETA: 0s - loss: 0.0323 - accuracy: 0.98 - ETA: 0s - loss: 0.0321 - accuracy: 0.98 - ETA: 0s - loss: 0.0324 - accuracy: 0.98 - 6s 1ms/step - loss: 0.0324 - accuracy: 0.9867 - val_loss: 0.8372 - val_accuracy: 0.7715\n",
      "Epoch 2/10\n",
      "5329/5329 [==============================] - ETA: 6s - loss: 0.0639 - accuracy: 0.96 - ETA: 5s - loss: 0.0255 - accuracy: 0.98 - ETA: 5s - loss: 0.0206 - accuracy: 0.99 - ETA: 5s - loss: 0.0207 - accuracy: 0.99 - ETA: 5s - loss: 0.0183 - accuracy: 0.99 - ETA: 5s - loss: 0.0190 - accuracy: 0.99 - ETA: 5s - loss: 0.0217 - accuracy: 0.99 - ETA: 5s - loss: 0.0215 - accuracy: 0.99 - ETA: 5s - loss: 0.0234 - accuracy: 0.98 - ETA: 5s - loss: 0.0238 - accuracy: 0.99 - ETA: 5s - loss: 0.0239 - accuracy: 0.98 - ETA: 5s - loss: 0.0248 - accuracy: 0.98 - ETA: 4s - loss: 0.0235 - accuracy: 0.99 - ETA: 4s - loss: 0.0253 - accuracy: 0.98 - ETA: 4s - loss: 0.0264 - accuracy: 0.98 - ETA: 4s - loss: 0.0255 - accuracy: 0.98 - ETA: 4s - loss: 0.0273 - accuracy: 0.98 - ETA: 4s - loss: 0.0269 - accuracy: 0.98 - ETA: 4s - loss: 0.0275 - accuracy: 0.98 - ETA: 4s - loss: 0.0272 - accuracy: 0.98 - ETA: 4s - loss: 0.0271 - accuracy: 0.98 - ETA: 4s - loss: 0.0279 - accuracy: 0.98 - ETA: 4s - loss: 0.0291 - accuracy: 0.98 - ETA: 4s - loss: 0.0311 - accuracy: 0.98 - ETA: 4s - loss: 0.0311 - accuracy: 0.98 - ETA: 4s - loss: 0.0310 - accuracy: 0.98 - ETA: 3s - loss: 0.0306 - accuracy: 0.98 - ETA: 3s - loss: 0.0297 - accuracy: 0.98 - ETA: 3s - loss: 0.0288 - accuracy: 0.98 - ETA: 3s - loss: 0.0300 - accuracy: 0.98 - ETA: 3s - loss: 0.0294 - accuracy: 0.98 - ETA: 3s - loss: 0.0297 - accuracy: 0.98 - ETA: 3s - loss: 0.0296 - accuracy: 0.98 - ETA: 3s - loss: 0.0299 - accuracy: 0.98 - ETA: 3s - loss: 0.0301 - accuracy: 0.98 - ETA: 3s - loss: 0.0294 - accuracy: 0.98 - ETA: 3s - loss: 0.0290 - accuracy: 0.98 - ETA: 3s - loss: 0.0288 - accuracy: 0.98 - ETA: 3s - loss: 0.0282 - accuracy: 0.98 - ETA: 3s - loss: 0.0283 - accuracy: 0.98 - ETA: 3s - loss: 0.0278 - accuracy: 0.98 - ETA: 2s - loss: 0.0285 - accuracy: 0.98 - ETA: 2s - loss: 0.0289 - accuracy: 0.98 - ETA: 2s - loss: 0.0285 - accuracy: 0.98 - ETA: 2s - loss: 0.0287 - accuracy: 0.98 - ETA: 2s - loss: 0.0298 - accuracy: 0.98 - ETA: 2s - loss: 0.0299 - accuracy: 0.98 - ETA: 2s - loss: 0.0293 - accuracy: 0.98 - ETA: 2s - loss: 0.0298 - accuracy: 0.98 - ETA: 2s - loss: 0.0315 - accuracy: 0.98 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0309 - accuracy: 0.98 - ETA: 2s - loss: 0.0312 - accuracy: 0.98 - ETA: 2s - loss: 0.0306 - accuracy: 0.98 - ETA: 2s - loss: 0.0307 - accuracy: 0.98 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0306 - accuracy: 0.98 - ETA: 1s - loss: 0.0307 - accuracy: 0.98 - ETA: 1s - loss: 0.0307 - accuracy: 0.98 - ETA: 1s - loss: 0.0311 - accuracy: 0.98 - ETA: 1s - loss: 0.0312 - accuracy: 0.98 - ETA: 1s - loss: 0.0312 - accuracy: 0.98 - ETA: 1s - loss: 0.0312 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0314 - accuracy: 0.98 - ETA: 1s - loss: 0.0315 - accuracy: 0.98 - ETA: 1s - loss: 0.0313 - accuracy: 0.98 - ETA: 1s - loss: 0.0312 - accuracy: 0.98 - ETA: 1s - loss: 0.0312 - accuracy: 0.98 - ETA: 0s - loss: 0.0316 - accuracy: 0.98 - ETA: 0s - loss: 0.0312 - accuracy: 0.98 - ETA: 0s - loss: 0.0309 - accuracy: 0.98 - ETA: 0s - loss: 0.0309 - accuracy: 0.98 - ETA: 0s - loss: 0.0313 - accuracy: 0.98 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0316 - accuracy: 0.98 - ETA: 0s - loss: 0.0315 - accuracy: 0.98 - ETA: 0s - loss: 0.0312 - accuracy: 0.98 - ETA: 0s - loss: 0.0311 - accuracy: 0.98 - ETA: 0s - loss: 0.0312 - accuracy: 0.98 - ETA: 0s - loss: 0.0311 - accuracy: 0.98 - ETA: 0s - loss: 0.0314 - accuracy: 0.98 - 6s 1ms/step - loss: 0.0320 - accuracy: 0.9852 - val_loss: 0.8644 - val_accuracy: 0.7658\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5329/5329 [==============================] - ETA: 5s - loss: 0.0186 - accuracy: 1.00 - ETA: 5s - loss: 0.0135 - accuracy: 1.00 - ETA: 5s - loss: 0.0172 - accuracy: 1.00 - ETA: 5s - loss: 0.0195 - accuracy: 0.99 - ETA: 5s - loss: 0.0271 - accuracy: 0.98 - ETA: 5s - loss: 0.0278 - accuracy: 0.98 - ETA: 5s - loss: 0.0270 - accuracy: 0.98 - ETA: 5s - loss: 0.0262 - accuracy: 0.98 - ETA: 5s - loss: 0.0250 - accuracy: 0.98 - ETA: 5s - loss: 0.0260 - accuracy: 0.98 - ETA: 5s - loss: 0.0250 - accuracy: 0.98 - ETA: 5s - loss: 0.0252 - accuracy: 0.98 - ETA: 4s - loss: 0.0246 - accuracy: 0.98 - ETA: 4s - loss: 0.0255 - accuracy: 0.98 - ETA: 4s - loss: 0.0252 - accuracy: 0.98 - ETA: 4s - loss: 0.0245 - accuracy: 0.98 - ETA: 4s - loss: 0.0252 - accuracy: 0.98 - ETA: 4s - loss: 0.0247 - accuracy: 0.98 - ETA: 4s - loss: 0.0299 - accuracy: 0.98 - ETA: 4s - loss: 0.0298 - accuracy: 0.98 - ETA: 4s - loss: 0.0286 - accuracy: 0.98 - ETA: 4s - loss: 0.0275 - accuracy: 0.98 - ETA: 4s - loss: 0.0270 - accuracy: 0.98 - ETA: 4s - loss: 0.0264 - accuracy: 0.98 - ETA: 4s - loss: 0.0268 - accuracy: 0.98 - ETA: 4s - loss: 0.0273 - accuracy: 0.98 - ETA: 3s - loss: 0.0267 - accuracy: 0.98 - ETA: 3s - loss: 0.0273 - accuracy: 0.98 - ETA: 3s - loss: 0.0278 - accuracy: 0.98 - ETA: 3s - loss: 0.0277 - accuracy: 0.98 - ETA: 3s - loss: 0.0278 - accuracy: 0.98 - ETA: 3s - loss: 0.0298 - accuracy: 0.98 - ETA: 3s - loss: 0.0296 - accuracy: 0.98 - ETA: 3s - loss: 0.0300 - accuracy: 0.98 - ETA: 3s - loss: 0.0306 - accuracy: 0.98 - ETA: 3s - loss: 0.0300 - accuracy: 0.98 - ETA: 3s - loss: 0.0299 - accuracy: 0.98 - ETA: 3s - loss: 0.0293 - accuracy: 0.98 - ETA: 3s - loss: 0.0302 - accuracy: 0.98 - ETA: 3s - loss: 0.0300 - accuracy: 0.98 - ETA: 2s - loss: 0.0303 - accuracy: 0.98 - ETA: 2s - loss: 0.0298 - accuracy: 0.98 - ETA: 2s - loss: 0.0303 - accuracy: 0.98 - ETA: 2s - loss: 0.0303 - accuracy: 0.98 - ETA: 2s - loss: 0.0305 - accuracy: 0.98 - ETA: 2s - loss: 0.0304 - accuracy: 0.98 - ETA: 2s - loss: 0.0300 - accuracy: 0.98 - ETA: 2s - loss: 0.0304 - accuracy: 0.98 - ETA: 2s - loss: 0.0312 - accuracy: 0.98 - ETA: 2s - loss: 0.0314 - accuracy: 0.98 - ETA: 2s - loss: 0.0309 - accuracy: 0.98 - ETA: 2s - loss: 0.0306 - accuracy: 0.98 - ETA: 2s - loss: 0.0309 - accuracy: 0.98 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0309 - accuracy: 0.98 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0302 - accuracy: 0.98 - ETA: 1s - loss: 0.0298 - accuracy: 0.98 - ETA: 1s - loss: 0.0299 - accuracy: 0.98 - ETA: 1s - loss: 0.0308 - accuracy: 0.98 - ETA: 1s - loss: 0.0310 - accuracy: 0.98 - ETA: 1s - loss: 0.0307 - accuracy: 0.98 - ETA: 1s - loss: 0.0308 - accuracy: 0.98 - ETA: 1s - loss: 0.0308 - accuracy: 0.98 - ETA: 1s - loss: 0.0308 - accuracy: 0.98 - ETA: 1s - loss: 0.0309 - accuracy: 0.98 - ETA: 0s - loss: 0.0310 - accuracy: 0.98 - ETA: 0s - loss: 0.0306 - accuracy: 0.98 - ETA: 0s - loss: 0.0305 - accuracy: 0.98 - ETA: 0s - loss: 0.0301 - accuracy: 0.98 - ETA: 0s - loss: 0.0301 - accuracy: 0.98 - ETA: 0s - loss: 0.0298 - accuracy: 0.98 - ETA: 0s - loss: 0.0302 - accuracy: 0.98 - ETA: 0s - loss: 0.0304 - accuracy: 0.98 - ETA: 0s - loss: 0.0301 - accuracy: 0.98 - ETA: 0s - loss: 0.0300 - accuracy: 0.98 - ETA: 0s - loss: 0.0302 - accuracy: 0.98 - ETA: 0s - loss: 0.0304 - accuracy: 0.98 - ETA: 0s - loss: 0.0306 - accuracy: 0.98 - ETA: 0s - loss: 0.0307 - accuracy: 0.98 - 6s 1ms/step - loss: 0.0309 - accuracy: 0.9861 - val_loss: 0.7976 - val_accuracy: 0.7710\n",
      "Epoch 4/10\n",
      "5329/5329 [==============================] - ETA: 5s - loss: 0.0297 - accuracy: 0.96 - ETA: 5s - loss: 0.0650 - accuracy: 0.96 - ETA: 5s - loss: 0.0490 - accuracy: 0.97 - ETA: 5s - loss: 0.0446 - accuracy: 0.98 - ETA: 5s - loss: 0.0375 - accuracy: 0.98 - ETA: 5s - loss: 0.0314 - accuracy: 0.98 - ETA: 5s - loss: 0.0299 - accuracy: 0.98 - ETA: 5s - loss: 0.0280 - accuracy: 0.98 - ETA: 5s - loss: 0.0268 - accuracy: 0.98 - ETA: 5s - loss: 0.0282 - accuracy: 0.98 - ETA: 5s - loss: 0.0287 - accuracy: 0.98 - ETA: 4s - loss: 0.0290 - accuracy: 0.98 - ETA: 4s - loss: 0.0326 - accuracy: 0.98 - ETA: 4s - loss: 0.0328 - accuracy: 0.98 - ETA: 4s - loss: 0.0308 - accuracy: 0.98 - ETA: 4s - loss: 0.0316 - accuracy: 0.98 - ETA: 4s - loss: 0.0304 - accuracy: 0.98 - ETA: 4s - loss: 0.0295 - accuracy: 0.98 - ETA: 4s - loss: 0.0295 - accuracy: 0.98 - ETA: 4s - loss: 0.0295 - accuracy: 0.98 - ETA: 4s - loss: 0.0290 - accuracy: 0.98 - ETA: 4s - loss: 0.0290 - accuracy: 0.98 - ETA: 4s - loss: 0.0288 - accuracy: 0.98 - ETA: 4s - loss: 0.0284 - accuracy: 0.98 - ETA: 4s - loss: 0.0282 - accuracy: 0.98 - ETA: 3s - loss: 0.0289 - accuracy: 0.98 - ETA: 3s - loss: 0.0304 - accuracy: 0.98 - ETA: 3s - loss: 0.0302 - accuracy: 0.98 - ETA: 3s - loss: 0.0309 - accuracy: 0.98 - ETA: 3s - loss: 0.0301 - accuracy: 0.98 - ETA: 3s - loss: 0.0311 - accuracy: 0.98 - ETA: 3s - loss: 0.0314 - accuracy: 0.98 - ETA: 3s - loss: 0.0313 - accuracy: 0.98 - ETA: 3s - loss: 0.0307 - accuracy: 0.98 - ETA: 3s - loss: 0.0308 - accuracy: 0.98 - ETA: 3s - loss: 0.0305 - accuracy: 0.98 - ETA: 3s - loss: 0.0302 - accuracy: 0.98 - ETA: 3s - loss: 0.0296 - accuracy: 0.98 - ETA: 3s - loss: 0.0292 - accuracy: 0.98 - ETA: 2s - loss: 0.0290 - accuracy: 0.98 - ETA: 2s - loss: 0.0288 - accuracy: 0.98 - ETA: 2s - loss: 0.0285 - accuracy: 0.98 - ETA: 2s - loss: 0.0283 - accuracy: 0.98 - ETA: 2s - loss: 0.0281 - accuracy: 0.98 - ETA: 2s - loss: 0.0281 - accuracy: 0.98 - ETA: 2s - loss: 0.0287 - accuracy: 0.98 - ETA: 2s - loss: 0.0285 - accuracy: 0.98 - ETA: 2s - loss: 0.0288 - accuracy: 0.98 - ETA: 2s - loss: 0.0285 - accuracy: 0.98 - ETA: 2s - loss: 0.0290 - accuracy: 0.98 - ETA: 2s - loss: 0.0286 - accuracy: 0.98 - ETA: 2s - loss: 0.0282 - accuracy: 0.98 - ETA: 2s - loss: 0.0282 - accuracy: 0.98 - ETA: 2s - loss: 0.0280 - accuracy: 0.98 - ETA: 1s - loss: 0.0277 - accuracy: 0.98 - ETA: 1s - loss: 0.0283 - accuracy: 0.98 - ETA: 1s - loss: 0.0280 - accuracy: 0.98 - ETA: 1s - loss: 0.0282 - accuracy: 0.98 - ETA: 1s - loss: 0.0280 - accuracy: 0.98 - ETA: 1s - loss: 0.0287 - accuracy: 0.98 - ETA: 1s - loss: 0.0285 - accuracy: 0.98 - ETA: 1s - loss: 0.0281 - accuracy: 0.98 - ETA: 1s - loss: 0.0282 - accuracy: 0.98 - ETA: 1s - loss: 0.0282 - accuracy: 0.98 - ETA: 1s - loss: 0.0282 - accuracy: 0.98 - ETA: 1s - loss: 0.0287 - accuracy: 0.98 - ETA: 1s - loss: 0.0285 - accuracy: 0.98 - ETA: 1s - loss: 0.0289 - accuracy: 0.98 - ETA: 1s - loss: 0.0291 - accuracy: 0.98 - ETA: 0s - loss: 0.0297 - accuracy: 0.98 - ETA: 0s - loss: 0.0294 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - ETA: 0s - loss: 0.0290 - accuracy: 0.98 - ETA: 0s - loss: 0.0288 - accuracy: 0.98 - ETA: 0s - loss: 0.0286 - accuracy: 0.98 - ETA: 0s - loss: 0.0288 - accuracy: 0.98 - ETA: 0s - loss: 0.0290 - accuracy: 0.98 - ETA: 0s - loss: 0.0288 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - ETA: 0s - loss: 0.0290 - accuracy: 0.98 - ETA: 0s - loss: 0.0294 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - 6s 1ms/step - loss: 0.0293 - accuracy: 0.9867 - val_loss: 0.8657 - val_accuracy: 0.7588\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5329/5329 [==============================] - ETA: 6s - loss: 0.0046 - accuracy: 1.00 - ETA: 6s - loss: 0.0047 - accuracy: 1.00 - ETA: 5s - loss: 0.0196 - accuracy: 0.98 - ETA: 5s - loss: 0.0195 - accuracy: 0.99 - ETA: 5s - loss: 0.0197 - accuracy: 0.99 - ETA: 5s - loss: 0.0213 - accuracy: 0.99 - ETA: 5s - loss: 0.0190 - accuracy: 0.99 - ETA: 5s - loss: 0.0236 - accuracy: 0.98 - ETA: 5s - loss: 0.0223 - accuracy: 0.98 - ETA: 5s - loss: 0.0207 - accuracy: 0.99 - ETA: 5s - loss: 0.0201 - accuracy: 0.99 - ETA: 5s - loss: 0.0199 - accuracy: 0.99 - ETA: 4s - loss: 0.0185 - accuracy: 0.99 - ETA: 4s - loss: 0.0183 - accuracy: 0.99 - ETA: 4s - loss: 0.0185 - accuracy: 0.99 - ETA: 4s - loss: 0.0191 - accuracy: 0.99 - ETA: 4s - loss: 0.0200 - accuracy: 0.99 - ETA: 4s - loss: 0.0193 - accuracy: 0.99 - ETA: 4s - loss: 0.0191 - accuracy: 0.99 - ETA: 4s - loss: 0.0187 - accuracy: 0.99 - ETA: 4s - loss: 0.0181 - accuracy: 0.99 - ETA: 4s - loss: 0.0182 - accuracy: 0.99 - ETA: 4s - loss: 0.0179 - accuracy: 0.99 - ETA: 4s - loss: 0.0204 - accuracy: 0.99 - ETA: 4s - loss: 0.0207 - accuracy: 0.99 - ETA: 3s - loss: 0.0202 - accuracy: 0.99 - ETA: 3s - loss: 0.0195 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0193 - accuracy: 0.99 - ETA: 3s - loss: 0.0188 - accuracy: 0.99 - ETA: 3s - loss: 0.0187 - accuracy: 0.99 - ETA: 3s - loss: 0.0189 - accuracy: 0.99 - ETA: 3s - loss: 0.0186 - accuracy: 0.99 - ETA: 3s - loss: 0.0192 - accuracy: 0.99 - ETA: 3s - loss: 0.0197 - accuracy: 0.99 - ETA: 3s - loss: 0.0202 - accuracy: 0.99 - ETA: 3s - loss: 0.0211 - accuracy: 0.99 - ETA: 3s - loss: 0.0208 - accuracy: 0.99 - ETA: 3s - loss: 0.0205 - accuracy: 0.99 - ETA: 3s - loss: 0.0215 - accuracy: 0.99 - ETA: 2s - loss: 0.0220 - accuracy: 0.99 - ETA: 2s - loss: 0.0221 - accuracy: 0.99 - ETA: 2s - loss: 0.0219 - accuracy: 0.99 - ETA: 2s - loss: 0.0218 - accuracy: 0.99 - ETA: 2s - loss: 0.0221 - accuracy: 0.99 - ETA: 2s - loss: 0.0224 - accuracy: 0.99 - ETA: 2s - loss: 0.0223 - accuracy: 0.99 - ETA: 2s - loss: 0.0223 - accuracy: 0.99 - ETA: 2s - loss: 0.0239 - accuracy: 0.98 - ETA: 2s - loss: 0.0257 - accuracy: 0.98 - ETA: 2s - loss: 0.0253 - accuracy: 0.98 - ETA: 2s - loss: 0.0265 - accuracy: 0.98 - ETA: 2s - loss: 0.0263 - accuracy: 0.98 - ETA: 2s - loss: 0.0262 - accuracy: 0.98 - ETA: 2s - loss: 0.0262 - accuracy: 0.98 - ETA: 1s - loss: 0.0263 - accuracy: 0.98 - ETA: 1s - loss: 0.0265 - accuracy: 0.98 - ETA: 1s - loss: 0.0268 - accuracy: 0.98 - ETA: 1s - loss: 0.0269 - accuracy: 0.98 - ETA: 1s - loss: 0.0270 - accuracy: 0.98 - ETA: 1s - loss: 0.0270 - accuracy: 0.98 - ETA: 1s - loss: 0.0276 - accuracy: 0.98 - ETA: 1s - loss: 0.0277 - accuracy: 0.98 - ETA: 1s - loss: 0.0275 - accuracy: 0.98 - ETA: 1s - loss: 0.0279 - accuracy: 0.98 - ETA: 1s - loss: 0.0282 - accuracy: 0.98 - ETA: 1s - loss: 0.0283 - accuracy: 0.98 - ETA: 1s - loss: 0.0283 - accuracy: 0.98 - ETA: 1s - loss: 0.0289 - accuracy: 0.98 - ETA: 0s - loss: 0.0289 - accuracy: 0.98 - ETA: 0s - loss: 0.0297 - accuracy: 0.98 - ETA: 0s - loss: 0.0300 - accuracy: 0.98 - ETA: 0s - loss: 0.0299 - accuracy: 0.98 - ETA: 0s - loss: 0.0299 - accuracy: 0.98 - ETA: 0s - loss: 0.0295 - accuracy: 0.98 - ETA: 0s - loss: 0.0299 - accuracy: 0.98 - ETA: 0s - loss: 0.0301 - accuracy: 0.98 - ETA: 0s - loss: 0.0298 - accuracy: 0.98 - ETA: 0s - loss: 0.0298 - accuracy: 0.98 - ETA: 0s - loss: 0.0300 - accuracy: 0.98 - ETA: 0s - loss: 0.0298 - accuracy: 0.98 - ETA: 0s - loss: 0.0295 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - 6s 1ms/step - loss: 0.0294 - accuracy: 0.9865 - val_loss: 0.9268 - val_accuracy: 0.7658\n",
      "Epoch 6/10\n",
      "5329/5329 [==============================] - ETA: 5s - loss: 0.0033 - accuracy: 1.00 - ETA: 5s - loss: 0.0031 - accuracy: 1.00 - ETA: 5s - loss: 0.0158 - accuracy: 0.98 - ETA: 5s - loss: 0.0136 - accuracy: 0.99 - ETA: 5s - loss: 0.0162 - accuracy: 0.98 - ETA: 5s - loss: 0.0202 - accuracy: 0.98 - ETA: 5s - loss: 0.0198 - accuracy: 0.99 - ETA: 5s - loss: 0.0264 - accuracy: 0.98 - ETA: 5s - loss: 0.0249 - accuracy: 0.98 - ETA: 5s - loss: 0.0257 - accuracy: 0.98 - ETA: 5s - loss: 0.0275 - accuracy: 0.98 - ETA: 5s - loss: 0.0256 - accuracy: 0.98 - ETA: 5s - loss: 0.0259 - accuracy: 0.99 - ETA: 4s - loss: 0.0247 - accuracy: 0.99 - ETA: 4s - loss: 0.0252 - accuracy: 0.99 - ETA: 4s - loss: 0.0253 - accuracy: 0.99 - ETA: 4s - loss: 0.0251 - accuracy: 0.99 - ETA: 4s - loss: 0.0258 - accuracy: 0.99 - ETA: 4s - loss: 0.0252 - accuracy: 0.98 - ETA: 4s - loss: 0.0247 - accuracy: 0.98 - ETA: 4s - loss: 0.0246 - accuracy: 0.99 - ETA: 4s - loss: 0.0274 - accuracy: 0.98 - ETA: 4s - loss: 0.0271 - accuracy: 0.98 - ETA: 4s - loss: 0.0279 - accuracy: 0.98 - ETA: 4s - loss: 0.0279 - accuracy: 0.98 - ETA: 4s - loss: 0.0275 - accuracy: 0.98 - ETA: 3s - loss: 0.0266 - accuracy: 0.98 - ETA: 3s - loss: 0.0262 - accuracy: 0.98 - ETA: 3s - loss: 0.0272 - accuracy: 0.98 - ETA: 3s - loss: 0.0275 - accuracy: 0.98 - ETA: 3s - loss: 0.0273 - accuracy: 0.98 - ETA: 3s - loss: 0.0270 - accuracy: 0.98 - ETA: 3s - loss: 0.0264 - accuracy: 0.98 - ETA: 3s - loss: 0.0271 - accuracy: 0.98 - ETA: 3s - loss: 0.0281 - accuracy: 0.98 - ETA: 3s - loss: 0.0284 - accuracy: 0.98 - ETA: 3s - loss: 0.0281 - accuracy: 0.98 - ETA: 3s - loss: 0.0295 - accuracy: 0.98 - ETA: 3s - loss: 0.0308 - accuracy: 0.98 - ETA: 3s - loss: 0.0309 - accuracy: 0.98 - ETA: 2s - loss: 0.0310 - accuracy: 0.98 - ETA: 2s - loss: 0.0308 - accuracy: 0.98 - ETA: 2s - loss: 0.0301 - accuracy: 0.98 - ETA: 2s - loss: 0.0301 - accuracy: 0.98 - ETA: 2s - loss: 0.0295 - accuracy: 0.98 - ETA: 2s - loss: 0.0297 - accuracy: 0.98 - ETA: 2s - loss: 0.0296 - accuracy: 0.98 - ETA: 2s - loss: 0.0298 - accuracy: 0.98 - ETA: 2s - loss: 0.0293 - accuracy: 0.98 - ETA: 2s - loss: 0.0294 - accuracy: 0.98 - ETA: 2s - loss: 0.0293 - accuracy: 0.98 - ETA: 2s - loss: 0.0293 - accuracy: 0.98 - ETA: 2s - loss: 0.0297 - accuracy: 0.98 - ETA: 2s - loss: 0.0300 - accuracy: 0.98 - ETA: 2s - loss: 0.0305 - accuracy: 0.98 - ETA: 1s - loss: 0.0303 - accuracy: 0.98 - ETA: 1s - loss: 0.0303 - accuracy: 0.98 - ETA: 1s - loss: 0.0300 - accuracy: 0.98 - ETA: 1s - loss: 0.0296 - accuracy: 0.98 - ETA: 1s - loss: 0.0292 - accuracy: 0.98 - ETA: 1s - loss: 0.0294 - accuracy: 0.98 - ETA: 1s - loss: 0.0293 - accuracy: 0.98 - ETA: 1s - loss: 0.0290 - accuracy: 0.98 - ETA: 1s - loss: 0.0287 - accuracy: 0.98 - ETA: 1s - loss: 0.0286 - accuracy: 0.98 - ETA: 1s - loss: 0.0284 - accuracy: 0.98 - ETA: 1s - loss: 0.0287 - accuracy: 0.98 - ETA: 1s - loss: 0.0290 - accuracy: 0.98 - ETA: 1s - loss: 0.0287 - accuracy: 0.98 - ETA: 0s - loss: 0.0284 - accuracy: 0.98 - ETA: 0s - loss: 0.0287 - accuracy: 0.98 - ETA: 0s - loss: 0.0289 - accuracy: 0.98 - ETA: 0s - loss: 0.0292 - accuracy: 0.98 - ETA: 0s - loss: 0.0294 - accuracy: 0.98 - ETA: 0s - loss: 0.0293 - accuracy: 0.98 - ETA: 0s - loss: 0.0298 - accuracy: 0.98 - ETA: 0s - loss: 0.0297 - accuracy: 0.98 - ETA: 0s - loss: 0.0293 - accuracy: 0.98 - ETA: 0s - loss: 0.0290 - accuracy: 0.98 - ETA: 0s - loss: 0.0286 - accuracy: 0.98 - ETA: 0s - loss: 0.0290 - accuracy: 0.98 - ETA: 0s - loss: 0.0289 - accuracy: 0.98 - ETA: 0s - loss: 0.0290 - accuracy: 0.98 - 6s 1ms/step - loss: 0.0288 - accuracy: 0.9856 - val_loss: 0.9270 - val_accuracy: 0.7623\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5329/5329 [==============================] - ETA: 5s - loss: 0.0366 - accuracy: 1.00 - ETA: 6s - loss: 0.0286 - accuracy: 0.98 - ETA: 6s - loss: 0.0275 - accuracy: 0.98 - ETA: 5s - loss: 0.0277 - accuracy: 0.98 - ETA: 5s - loss: 0.0298 - accuracy: 0.98 - ETA: 5s - loss: 0.0294 - accuracy: 0.98 - ETA: 5s - loss: 0.0271 - accuracy: 0.98 - ETA: 5s - loss: 0.0279 - accuracy: 0.98 - ETA: 5s - loss: 0.0264 - accuracy: 0.98 - ETA: 5s - loss: 0.0261 - accuracy: 0.98 - ETA: 5s - loss: 0.0268 - accuracy: 0.98 - ETA: 4s - loss: 0.0250 - accuracy: 0.98 - ETA: 4s - loss: 0.0232 - accuracy: 0.98 - ETA: 4s - loss: 0.0228 - accuracy: 0.98 - ETA: 4s - loss: 0.0225 - accuracy: 0.98 - ETA: 4s - loss: 0.0235 - accuracy: 0.98 - ETA: 4s - loss: 0.0237 - accuracy: 0.98 - ETA: 4s - loss: 0.0227 - accuracy: 0.98 - ETA: 4s - loss: 0.0223 - accuracy: 0.98 - ETA: 4s - loss: 0.0224 - accuracy: 0.98 - ETA: 4s - loss: 0.0216 - accuracy: 0.98 - ETA: 4s - loss: 0.0208 - accuracy: 0.98 - ETA: 4s - loss: 0.0216 - accuracy: 0.98 - ETA: 4s - loss: 0.0218 - accuracy: 0.98 - ETA: 3s - loss: 0.0250 - accuracy: 0.98 - ETA: 3s - loss: 0.0264 - accuracy: 0.98 - ETA: 3s - loss: 0.0260 - accuracy: 0.98 - ETA: 3s - loss: 0.0261 - accuracy: 0.98 - ETA: 3s - loss: 0.0263 - accuracy: 0.98 - ETA: 3s - loss: 0.0265 - accuracy: 0.98 - ETA: 3s - loss: 0.0260 - accuracy: 0.98 - ETA: 3s - loss: 0.0253 - accuracy: 0.98 - ETA: 3s - loss: 0.0257 - accuracy: 0.98 - ETA: 3s - loss: 0.0251 - accuracy: 0.98 - ETA: 3s - loss: 0.0250 - accuracy: 0.98 - ETA: 3s - loss: 0.0253 - accuracy: 0.98 - ETA: 3s - loss: 0.0253 - accuracy: 0.98 - ETA: 3s - loss: 0.0253 - accuracy: 0.98 - ETA: 3s - loss: 0.0255 - accuracy: 0.98 - ETA: 2s - loss: 0.0254 - accuracy: 0.98 - ETA: 2s - loss: 0.0252 - accuracy: 0.98 - ETA: 2s - loss: 0.0250 - accuracy: 0.98 - ETA: 2s - loss: 0.0256 - accuracy: 0.98 - ETA: 2s - loss: 0.0251 - accuracy: 0.98 - ETA: 2s - loss: 0.0249 - accuracy: 0.98 - ETA: 2s - loss: 0.0246 - accuracy: 0.98 - ETA: 2s - loss: 0.0241 - accuracy: 0.98 - ETA: 2s - loss: 0.0247 - accuracy: 0.98 - ETA: 2s - loss: 0.0245 - accuracy: 0.98 - ETA: 2s - loss: 0.0250 - accuracy: 0.98 - ETA: 2s - loss: 0.0255 - accuracy: 0.98 - ETA: 2s - loss: 0.0262 - accuracy: 0.98 - ETA: 2s - loss: 0.0258 - accuracy: 0.98 - ETA: 2s - loss: 0.0260 - accuracy: 0.98 - ETA: 1s - loss: 0.0271 - accuracy: 0.98 - ETA: 1s - loss: 0.0271 - accuracy: 0.98 - ETA: 1s - loss: 0.0269 - accuracy: 0.98 - ETA: 1s - loss: 0.0269 - accuracy: 0.98 - ETA: 1s - loss: 0.0270 - accuracy: 0.98 - ETA: 1s - loss: 0.0268 - accuracy: 0.98 - ETA: 1s - loss: 0.0266 - accuracy: 0.98 - ETA: 1s - loss: 0.0265 - accuracy: 0.98 - ETA: 1s - loss: 0.0267 - accuracy: 0.98 - ETA: 1s - loss: 0.0269 - accuracy: 0.98 - ETA: 1s - loss: 0.0268 - accuracy: 0.98 - ETA: 1s - loss: 0.0267 - accuracy: 0.98 - ETA: 1s - loss: 0.0267 - accuracy: 0.98 - ETA: 1s - loss: 0.0269 - accuracy: 0.98 - ETA: 1s - loss: 0.0266 - accuracy: 0.98 - ETA: 0s - loss: 0.0270 - accuracy: 0.98 - ETA: 0s - loss: 0.0277 - accuracy: 0.98 - ETA: 0s - loss: 0.0285 - accuracy: 0.98 - ETA: 0s - loss: 0.0282 - accuracy: 0.98 - ETA: 0s - loss: 0.0283 - accuracy: 0.98 - ETA: 0s - loss: 0.0282 - accuracy: 0.98 - ETA: 0s - loss: 0.0284 - accuracy: 0.98 - ETA: 0s - loss: 0.0280 - accuracy: 0.98 - ETA: 0s - loss: 0.0280 - accuracy: 0.98 - ETA: 0s - loss: 0.0280 - accuracy: 0.98 - ETA: 0s - loss: 0.0283 - accuracy: 0.98 - ETA: 0s - loss: 0.0286 - accuracy: 0.98 - ETA: 0s - loss: 0.0287 - accuracy: 0.98 - ETA: 0s - loss: 0.0286 - accuracy: 0.98 - 6s 1ms/step - loss: 0.0284 - accuracy: 0.9854 - val_loss: 0.9281 - val_accuracy: 0.7618\n",
      "Epoch 8/10\n",
      "5329/5329 [==============================] - ETA: 6s - loss: 0.0454 - accuracy: 0.96 - ETA: 5s - loss: 0.0525 - accuracy: 0.96 - ETA: 5s - loss: 0.0412 - accuracy: 0.97 - ETA: 5s - loss: 0.0364 - accuracy: 0.97 - ETA: 5s - loss: 0.0309 - accuracy: 0.98 - ETA: 5s - loss: 0.0341 - accuracy: 0.98 - ETA: 5s - loss: 0.0316 - accuracy: 0.98 - ETA: 5s - loss: 0.0330 - accuracy: 0.98 - ETA: 5s - loss: 0.0330 - accuracy: 0.98 - ETA: 5s - loss: 0.0315 - accuracy: 0.98 - ETA: 5s - loss: 0.0306 - accuracy: 0.98 - ETA: 5s - loss: 0.0339 - accuracy: 0.98 - ETA: 4s - loss: 0.0322 - accuracy: 0.98 - ETA: 4s - loss: 0.0304 - accuracy: 0.98 - ETA: 4s - loss: 0.0286 - accuracy: 0.98 - ETA: 4s - loss: 0.0282 - accuracy: 0.98 - ETA: 4s - loss: 0.0270 - accuracy: 0.98 - ETA: 4s - loss: 0.0260 - accuracy: 0.98 - ETA: 4s - loss: 0.0253 - accuracy: 0.98 - ETA: 4s - loss: 0.0251 - accuracy: 0.98 - ETA: 4s - loss: 0.0242 - accuracy: 0.98 - ETA: 4s - loss: 0.0261 - accuracy: 0.98 - ETA: 4s - loss: 0.0267 - accuracy: 0.98 - ETA: 4s - loss: 0.0257 - accuracy: 0.98 - ETA: 4s - loss: 0.0250 - accuracy: 0.98 - ETA: 3s - loss: 0.0261 - accuracy: 0.98 - ETA: 3s - loss: 0.0260 - accuracy: 0.98 - ETA: 3s - loss: 0.0251 - accuracy: 0.98 - ETA: 3s - loss: 0.0249 - accuracy: 0.98 - ETA: 3s - loss: 0.0241 - accuracy: 0.98 - ETA: 3s - loss: 0.0235 - accuracy: 0.98 - ETA: 3s - loss: 0.0231 - accuracy: 0.98 - ETA: 3s - loss: 0.0233 - accuracy: 0.98 - ETA: 3s - loss: 0.0235 - accuracy: 0.98 - ETA: 3s - loss: 0.0230 - accuracy: 0.98 - ETA: 3s - loss: 0.0228 - accuracy: 0.98 - ETA: 3s - loss: 0.0226 - accuracy: 0.98 - ETA: 3s - loss: 0.0236 - accuracy: 0.98 - ETA: 3s - loss: 0.0238 - accuracy: 0.98 - ETA: 3s - loss: 0.0240 - accuracy: 0.98 - ETA: 2s - loss: 0.0240 - accuracy: 0.98 - ETA: 2s - loss: 0.0236 - accuracy: 0.98 - ETA: 2s - loss: 0.0240 - accuracy: 0.98 - ETA: 2s - loss: 0.0240 - accuracy: 0.98 - ETA: 2s - loss: 0.0255 - accuracy: 0.98 - ETA: 2s - loss: 0.0252 - accuracy: 0.98 - ETA: 2s - loss: 0.0253 - accuracy: 0.98 - ETA: 2s - loss: 0.0249 - accuracy: 0.98 - ETA: 2s - loss: 0.0249 - accuracy: 0.98 - ETA: 2s - loss: 0.0262 - accuracy: 0.98 - ETA: 2s - loss: 0.0264 - accuracy: 0.98 - ETA: 2s - loss: 0.0266 - accuracy: 0.98 - ETA: 2s - loss: 0.0269 - accuracy: 0.98 - ETA: 2s - loss: 0.0270 - accuracy: 0.98 - ETA: 1s - loss: 0.0269 - accuracy: 0.98 - ETA: 1s - loss: 0.0264 - accuracy: 0.98 - ETA: 1s - loss: 0.0265 - accuracy: 0.98 - ETA: 1s - loss: 0.0269 - accuracy: 0.98 - ETA: 1s - loss: 0.0266 - accuracy: 0.98 - ETA: 1s - loss: 0.0271 - accuracy: 0.98 - ETA: 1s - loss: 0.0273 - accuracy: 0.98 - ETA: 1s - loss: 0.0273 - accuracy: 0.98 - ETA: 1s - loss: 0.0271 - accuracy: 0.98 - ETA: 1s - loss: 0.0267 - accuracy: 0.98 - ETA: 1s - loss: 0.0269 - accuracy: 0.98 - ETA: 1s - loss: 0.0267 - accuracy: 0.98 - ETA: 1s - loss: 0.0267 - accuracy: 0.98 - ETA: 1s - loss: 0.0267 - accuracy: 0.98 - ETA: 1s - loss: 0.0266 - accuracy: 0.98 - ETA: 0s - loss: 0.0265 - accuracy: 0.98 - ETA: 0s - loss: 0.0261 - accuracy: 0.98 - ETA: 0s - loss: 0.0263 - accuracy: 0.98 - ETA: 0s - loss: 0.0260 - accuracy: 0.98 - ETA: 0s - loss: 0.0263 - accuracy: 0.98 - ETA: 0s - loss: 0.0265 - accuracy: 0.98 - ETA: 0s - loss: 0.0268 - accuracy: 0.98 - ETA: 0s - loss: 0.0267 - accuracy: 0.98 - ETA: 0s - loss: 0.0266 - accuracy: 0.98 - ETA: 0s - loss: 0.0267 - accuracy: 0.98 - ETA: 0s - loss: 0.0268 - accuracy: 0.98 - ETA: 0s - loss: 0.0268 - accuracy: 0.98 - ETA: 0s - loss: 0.0271 - accuracy: 0.98 - ETA: 0s - loss: 0.0271 - accuracy: 0.98 - 6s 1ms/step - loss: 0.0274 - accuracy: 0.9869 - val_loss: 0.8645 - val_accuracy: 0.7636\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5329/5329 [==============================] - ETA: 5s - loss: 0.0269 - accuracy: 1.00 - ETA: 5s - loss: 0.0179 - accuracy: 1.00 - ETA: 5s - loss: 0.0378 - accuracy: 0.98 - ETA: 5s - loss: 0.0380 - accuracy: 0.98 - ETA: 5s - loss: 0.0393 - accuracy: 0.97 - ETA: 5s - loss: 0.0343 - accuracy: 0.98 - ETA: 5s - loss: 0.0328 - accuracy: 0.98 - ETA: 5s - loss: 0.0330 - accuracy: 0.98 - ETA: 5s - loss: 0.0345 - accuracy: 0.98 - ETA: 5s - loss: 0.0347 - accuracy: 0.98 - ETA: 4s - loss: 0.0319 - accuracy: 0.98 - ETA: 4s - loss: 0.0306 - accuracy: 0.98 - ETA: 4s - loss: 0.0284 - accuracy: 0.98 - ETA: 4s - loss: 0.0278 - accuracy: 0.98 - ETA: 4s - loss: 0.0264 - accuracy: 0.99 - ETA: 4s - loss: 0.0267 - accuracy: 0.98 - ETA: 4s - loss: 0.0258 - accuracy: 0.98 - ETA: 4s - loss: 0.0248 - accuracy: 0.99 - ETA: 4s - loss: 0.0262 - accuracy: 0.98 - ETA: 4s - loss: 0.0258 - accuracy: 0.98 - ETA: 4s - loss: 0.0255 - accuracy: 0.98 - ETA: 4s - loss: 0.0247 - accuracy: 0.98 - ETA: 4s - loss: 0.0243 - accuracy: 0.98 - ETA: 4s - loss: 0.0252 - accuracy: 0.98 - ETA: 3s - loss: 0.0246 - accuracy: 0.98 - ETA: 3s - loss: 0.0255 - accuracy: 0.98 - ETA: 3s - loss: 0.0259 - accuracy: 0.98 - ETA: 3s - loss: 0.0251 - accuracy: 0.98 - ETA: 3s - loss: 0.0245 - accuracy: 0.98 - ETA: 3s - loss: 0.0246 - accuracy: 0.98 - ETA: 3s - loss: 0.0243 - accuracy: 0.98 - ETA: 3s - loss: 0.0250 - accuracy: 0.98 - ETA: 3s - loss: 0.0254 - accuracy: 0.98 - ETA: 3s - loss: 0.0252 - accuracy: 0.98 - ETA: 3s - loss: 0.0251 - accuracy: 0.98 - ETA: 3s - loss: 0.0245 - accuracy: 0.98 - ETA: 3s - loss: 0.0240 - accuracy: 0.98 - ETA: 3s - loss: 0.0237 - accuracy: 0.98 - ETA: 3s - loss: 0.0244 - accuracy: 0.98 - ETA: 2s - loss: 0.0241 - accuracy: 0.98 - ETA: 2s - loss: 0.0246 - accuracy: 0.98 - ETA: 2s - loss: 0.0249 - accuracy: 0.98 - ETA: 2s - loss: 0.0247 - accuracy: 0.98 - ETA: 2s - loss: 0.0253 - accuracy: 0.98 - ETA: 2s - loss: 0.0251 - accuracy: 0.98 - ETA: 2s - loss: 0.0249 - accuracy: 0.98 - ETA: 2s - loss: 0.0246 - accuracy: 0.98 - ETA: 2s - loss: 0.0242 - accuracy: 0.98 - ETA: 2s - loss: 0.0244 - accuracy: 0.98 - ETA: 2s - loss: 0.0240 - accuracy: 0.98 - ETA: 2s - loss: 0.0243 - accuracy: 0.98 - ETA: 2s - loss: 0.0240 - accuracy: 0.98 - ETA: 2s - loss: 0.0239 - accuracy: 0.98 - ETA: 2s - loss: 0.0240 - accuracy: 0.98 - ETA: 1s - loss: 0.0251 - accuracy: 0.98 - ETA: 1s - loss: 0.0250 - accuracy: 0.98 - ETA: 1s - loss: 0.0251 - accuracy: 0.98 - ETA: 1s - loss: 0.0260 - accuracy: 0.98 - ETA: 1s - loss: 0.0261 - accuracy: 0.98 - ETA: 1s - loss: 0.0262 - accuracy: 0.98 - ETA: 1s - loss: 0.0261 - accuracy: 0.98 - ETA: 1s - loss: 0.0258 - accuracy: 0.98 - ETA: 1s - loss: 0.0259 - accuracy: 0.98 - ETA: 1s - loss: 0.0261 - accuracy: 0.98 - ETA: 1s - loss: 0.0264 - accuracy: 0.98 - ETA: 1s - loss: 0.0264 - accuracy: 0.98 - ETA: 1s - loss: 0.0261 - accuracy: 0.98 - ETA: 1s - loss: 0.0260 - accuracy: 0.98 - ETA: 1s - loss: 0.0257 - accuracy: 0.98 - ETA: 0s - loss: 0.0256 - accuracy: 0.98 - ETA: 0s - loss: 0.0255 - accuracy: 0.98 - ETA: 0s - loss: 0.0258 - accuracy: 0.98 - ETA: 0s - loss: 0.0254 - accuracy: 0.98 - ETA: 0s - loss: 0.0253 - accuracy: 0.98 - ETA: 0s - loss: 0.0254 - accuracy: 0.98 - ETA: 0s - loss: 0.0253 - accuracy: 0.98 - ETA: 0s - loss: 0.0257 - accuracy: 0.98 - ETA: 0s - loss: 0.0254 - accuracy: 0.98 - ETA: 0s - loss: 0.0253 - accuracy: 0.98 - ETA: 0s - loss: 0.0253 - accuracy: 0.98 - ETA: 0s - loss: 0.0254 - accuracy: 0.98 - ETA: 0s - loss: 0.0257 - accuracy: 0.98 - ETA: 0s - loss: 0.0260 - accuracy: 0.98 - 6s 1ms/step - loss: 0.0259 - accuracy: 0.9869 - val_loss: 0.9732 - val_accuracy: 0.7640\n",
      "Epoch 10/10\n",
      "5329/5329 [==============================] - ETA: 5s - loss: 0.0822 - accuracy: 0.93 - ETA: 5s - loss: 0.0378 - accuracy: 0.96 - ETA: 5s - loss: 0.0296 - accuracy: 0.97 - ETA: 5s - loss: 0.0223 - accuracy: 0.98 - ETA: 5s - loss: 0.0194 - accuracy: 0.98 - ETA: 5s - loss: 0.0235 - accuracy: 0.98 - ETA: 5s - loss: 0.0215 - accuracy: 0.98 - ETA: 5s - loss: 0.0206 - accuracy: 0.98 - ETA: 5s - loss: 0.0204 - accuracy: 0.98 - ETA: 5s - loss: 0.0194 - accuracy: 0.98 - ETA: 4s - loss: 0.0190 - accuracy: 0.98 - ETA: 4s - loss: 0.0176 - accuracy: 0.98 - ETA: 4s - loss: 0.0172 - accuracy: 0.99 - ETA: 4s - loss: 0.0161 - accuracy: 0.99 - ETA: 4s - loss: 0.0166 - accuracy: 0.99 - ETA: 4s - loss: 0.0165 - accuracy: 0.99 - ETA: 4s - loss: 0.0167 - accuracy: 0.99 - ETA: 4s - loss: 0.0177 - accuracy: 0.99 - ETA: 4s - loss: 0.0169 - accuracy: 0.99 - ETA: 4s - loss: 0.0170 - accuracy: 0.99 - ETA: 4s - loss: 0.0164 - accuracy: 0.99 - ETA: 4s - loss: 0.0174 - accuracy: 0.98 - ETA: 4s - loss: 0.0181 - accuracy: 0.98 - ETA: 4s - loss: 0.0190 - accuracy: 0.98 - ETA: 3s - loss: 0.0183 - accuracy: 0.98 - ETA: 3s - loss: 0.0193 - accuracy: 0.98 - ETA: 3s - loss: 0.0190 - accuracy: 0.98 - ETA: 3s - loss: 0.0210 - accuracy: 0.98 - ETA: 3s - loss: 0.0205 - accuracy: 0.98 - ETA: 3s - loss: 0.0210 - accuracy: 0.98 - ETA: 3s - loss: 0.0211 - accuracy: 0.98 - ETA: 3s - loss: 0.0212 - accuracy: 0.98 - ETA: 3s - loss: 0.0212 - accuracy: 0.98 - ETA: 3s - loss: 0.0211 - accuracy: 0.98 - ETA: 3s - loss: 0.0210 - accuracy: 0.98 - ETA: 3s - loss: 0.0215 - accuracy: 0.98 - ETA: 3s - loss: 0.0224 - accuracy: 0.98 - ETA: 3s - loss: 0.0222 - accuracy: 0.98 - ETA: 3s - loss: 0.0225 - accuracy: 0.98 - ETA: 2s - loss: 0.0223 - accuracy: 0.98 - ETA: 2s - loss: 0.0230 - accuracy: 0.98 - ETA: 2s - loss: 0.0226 - accuracy: 0.98 - ETA: 2s - loss: 0.0222 - accuracy: 0.98 - ETA: 2s - loss: 0.0224 - accuracy: 0.98 - ETA: 2s - loss: 0.0219 - accuracy: 0.98 - ETA: 2s - loss: 0.0218 - accuracy: 0.98 - ETA: 2s - loss: 0.0218 - accuracy: 0.98 - ETA: 2s - loss: 0.0219 - accuracy: 0.98 - ETA: 2s - loss: 0.0222 - accuracy: 0.98 - ETA: 2s - loss: 0.0225 - accuracy: 0.98 - ETA: 2s - loss: 0.0237 - accuracy: 0.98 - ETA: 2s - loss: 0.0241 - accuracy: 0.98 - ETA: 2s - loss: 0.0254 - accuracy: 0.98 - ETA: 2s - loss: 0.0251 - accuracy: 0.98 - ETA: 1s - loss: 0.0258 - accuracy: 0.98 - ETA: 1s - loss: 0.0258 - accuracy: 0.98 - ETA: 1s - loss: 0.0262 - accuracy: 0.98 - ETA: 1s - loss: 0.0265 - accuracy: 0.98 - ETA: 1s - loss: 0.0270 - accuracy: 0.98 - ETA: 1s - loss: 0.0274 - accuracy: 0.98 - ETA: 1s - loss: 0.0270 - accuracy: 0.98 - ETA: 1s - loss: 0.0268 - accuracy: 0.98 - ETA: 1s - loss: 0.0275 - accuracy: 0.98 - ETA: 1s - loss: 0.0278 - accuracy: 0.98 - ETA: 1s - loss: 0.0279 - accuracy: 0.98 - ETA: 1s - loss: 0.0283 - accuracy: 0.98 - ETA: 1s - loss: 0.0282 - accuracy: 0.98 - ETA: 1s - loss: 0.0278 - accuracy: 0.98 - ETA: 1s - loss: 0.0286 - accuracy: 0.98 - ETA: 0s - loss: 0.0285 - accuracy: 0.98 - ETA: 0s - loss: 0.0284 - accuracy: 0.98 - ETA: 0s - loss: 0.0282 - accuracy: 0.98 - ETA: 0s - loss: 0.0281 - accuracy: 0.98 - ETA: 0s - loss: 0.0282 - accuracy: 0.98 - ETA: 0s - loss: 0.0280 - accuracy: 0.98 - ETA: 0s - loss: 0.0278 - accuracy: 0.98 - ETA: 0s - loss: 0.0278 - accuracy: 0.98 - ETA: 0s - loss: 0.0275 - accuracy: 0.98 - ETA: 0s - loss: 0.0274 - accuracy: 0.98 - ETA: 0s - loss: 0.0274 - accuracy: 0.98 - ETA: 0s - loss: 0.0274 - accuracy: 0.98 - ETA: 0s - loss: 0.0271 - accuracy: 0.98 - ETA: 0s - loss: 0.0271 - accuracy: 0.98 - 6s 1ms/step - loss: 0.0273 - accuracy: 0.9856 - val_loss: 0.9281 - val_accuracy: 0.7636\n"
     ]
    }
   ],
   "source": [
    "r = model.fit(X_train_data,y_train,epochs=10,verbose=1,\n",
    "         validation_data=(X_val_data,y_val),\n",
    "         callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-80af1224f205>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcv_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[1;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[0;32m   1969\u001b[0m     \"\"\"\n\u001b[0;32m   1970\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1971\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1973\u001b[0m     \u001b[0mlabels_given\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 90\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": [
    "cv_scores = classification_report(y_hat,y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1c7329ecd48>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(r.history['accuracy'],label = 'Training_Accuracy')\n",
    "plt.plot(r.history['val_accuracy'],label = 'val_Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(output_dir+\"/weights.03.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#''.join(index_word[id] for id in X_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_y_hat = []\n",
    "for y in y_hat:\n",
    "    float_y_hat.append(y[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydf = pd.DataFrame(list(zip(float_y_hat,y_val)),columns=['y_hat','y_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_hat</th>\n",
       "      <th>y_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.017360</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.648535</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.152716</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      y_hat  y_val\n",
       "0  0.017360      1\n",
       "1  0.648535      0\n",
       "2  0.072800      1\n",
       "3  0.000778      0\n",
       "4  0.152716      0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and binary targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-13b7f93605b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1097\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1098\u001b[0m                        \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1099\u001b[1;33m                        zero_division=zero_division)\n\u001b[0m\u001b[0;32m   1100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1224\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1226\u001b[1;33m                                                  zero_division=zero_division)\n\u001b[0m\u001b[0;32m   1227\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1482\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[1;32m-> 1484\u001b[1;33m                                     pos_label)\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1299\u001b[0m                          str(average_options))\n\u001b[0;32m   1300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1301\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1302\u001b[0m     \u001b[0mpresent_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maverage\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'binary'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[1;32m---> 90\u001b[1;33m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and binary targets"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 64)           320000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                409664    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 729,729\n",
      "Trainable params: 729,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#classifier Architecture:\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_unique_words,n_dim,input_length=max_twit_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_dense,activation='relu'))\n",
    "model.add(Dropout(dropout))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling the classifier\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint after each epoch\n",
    "\n",
    "modelcheckpoint = ModelCheckpoint(filepath=output_dir+\"/weights.{epoch:02d}.hdf5\")\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected embedding_1_input to have shape (100,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-888f8c7871f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1,\n\u001b[0;32m      4\u001b[0m          \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m          callbacks=[modelcheckpoint])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1154\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m   1155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    143\u001b[0m                             \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                             str(data_shape))\n\u001b[0m\u001b[0;32m    146\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected embedding_1_input to have shape (100,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "#fitting classifier\n",
    "\n",
    "model.fit(X_train,y_train,batch_size=batch_size,epochs=epochs,verbose=1,\n",
    "         validation_data=(X_val,y_val),\n",
    "         callbacks=[modelcheckpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
